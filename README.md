# ğŸ—£ï¸ Aphasia Conversation Trainer (Proof of Concept)

A local **multimodal conversational trainer** for people with aphasia â€” designed to help practice everyday Danish scenarios such as shopping, ordering food, or small talk.  
Built with **Streamlit**, **Ollama**, and **Whisper**, and intended to eventually include **kokoro-tts** for natural speech output.

---

## ğŸš€ Quick Start

### 1. Clone or open the project folder
```bash
cd DailyLifeRoleplay
```

### 2. Create a virtual environment
```bash
python3 -m venv .venv
source .venv/bin/activate
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

### 4. Start required backend services

#### ğŸ§  Ollama (local LLM)
Make sure Ollama is installed and running:

```bash
ollama serve
ollama pull llama3.1:8b-instruct
```

#### ğŸ¤ Whisper realtime transcriber
Run the provided speech service (realtime partial + final transcription):

```bash
python realtime_transcriber.py
```

It should expose an endpoint such as:
```
http://localhost:9000/final
```
that returns JSON:
```json
{"text": "Jeg vil gerne kÃ¸be noget kÃ¸d."}
```

### 5. Launch the Streamlit interface
```bash
python -m streamlit run app.py
```

The app will open in your browser (default: [http://localhost:8501](http://localhost:8501)).

---

## ğŸ§© Features

- âœ… Always-listening **speech input** (can be toggled off)  
- ğŸ’¬ **Dual input modes:** text or pictorial (emoji for now)  
- ğŸ” **Up to 10 clickable response suggestions** per turn  
- ğŸ”Š **Spoken replies** via kokoro-tts (placeholder: `pyttsx3`)  
- ğŸ§± Built for **local operation** and full privacy  

---

## ğŸ§  Design Overview

```
ğŸ¤ Microphone â†’ realtime_transcriber.py â†’ JSON (partials/final)
                       â†“
         Streamlit frontend (active listening)
                       â†“
          Ollama LLM (llama3.1:8b-instruct)
                       â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Assistant reply + 10 candidate responses     â”‚
  â”‚ (text + emoji, later pictograms or images)   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
           Kokoro-TTS reads replies aloud
```

---

## ğŸ§° Project Structure

```
DailyLifeRoleplay/
â”œâ”€â”€ app.py                  # Streamlit PoC
â”œâ”€â”€ realtime_transcriber.py # Whisper input backend
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .venv/                  # Virtual environment (local)
```

---

## âš™ï¸ Requirements

| Component | Description | Notes |
|------------|--------------|-------|
| **Python â‰¥ 3.9** | Core runtime | Tested on macOS |
| **Ollama** | Local LLM serving (`llama3.1:8b-instruct`) | [ollama.ai/download](https://ollama.ai/download) |
| **Whisper** | Realtime STT (`realtime_transcriber.py`) | ggerganov/whisper.cpp or faster-whisper |
| **Kokoro-TTS** | Natural speech output | Optional â€” placeholder uses `pyttsx3` |
| **Streamlit** | Frontend UI | Installed via `requirements.txt` |

---

## ğŸ§­ Next Steps

- ğŸ”„ Replace emoji with **real pictograms** or **generated images**
- ğŸ–±ï¸ Add **hover-to-speak** feature for response tiles
- ğŸ”ˆ Integrate **kokoro-tts** playback via API
- ğŸ§© Add **custom Modelfile** for aphasia-friendly prompting
- ğŸ§  Optional: persist user progress or scenario tracking

---

## ğŸ“„ License

MIT License â€” for research and educational use.
